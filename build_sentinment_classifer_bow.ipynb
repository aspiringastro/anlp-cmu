{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c95bbe9-0ddb-41f0-bf82-5267a375d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e0126eb-37ad-424a-b26c-22c8f59b0356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8544 8544\n",
      "1101 1101\n",
      "2210 2210\n"
     ]
    }
   ],
   "source": [
    " # Load the dataset\n",
    "    \n",
    "start_tok = '<S>'\n",
    "end_tok = '<E>'\n",
    "unk_tok = '<U>'\n",
    "    \n",
    "def load_dataset(file):\n",
    "    reviews = open(file, 'r').read().splitlines()\n",
    "    X = []\n",
    "    Y = []\n",
    "    for review in reviews:\n",
    "        label, text = review.strip().split('|||')\n",
    "        # Start and End tokens are added - in the final set, add space and unknown tokens as well\n",
    "        X.append([ start_tok ] + text.strip().split(' ') + [ end_tok ])\n",
    "        Y.append(int(label))\n",
    "    print(len(X), len(Y))\n",
    "    return X, Y\n",
    "        \n",
    "Xtrw, Ytrw = load_dataset('data/sst-sentiment-text-threeclass/train.txt')\n",
    "Xdevw, Ydevw = load_dataset('data/sst-sentiment-text-threeclass/dev.txt')\n",
    "Xtew, Ytew = load_dataset('data/sst-sentiment-text-threeclass/test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22ecab5d-7fa2-4b14-a303-3dada2529969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868033\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "words = { word for x in Xtr for word in x }\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(sorted(list(words)))}\n",
    "stoi[unk_tok] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "wordset_size = len(itos)\n",
    "print(wordset_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6dbcc7f-a5a7-4450-b720-34b0f883733b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: -1, 1: 0, 2: 1},\n",
       " {-1: 0, 0: 1, 1: 2},\n",
       " {0: tensor([0., 0., 1.]), 1: tensor([1., 0., 0.]), 2: tensor([0., 1., 0.])})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readable_labels = {\n",
    "    -1 : 'negative',\n",
    "     0 : 'neutral',\n",
    "     1 : 'positive'\n",
    "}\n",
    "\n",
    "readable_labels_size = len(readable_labels)\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return torch.eye(num_classes, dtype=torch.float32)[y]\n",
    "\n",
    "itol = { i:v for i,v in enumerate(readable_labels) }\n",
    "ltoi = { v:i for i,v in itol.items() }\n",
    "itoc = { i:to_categorical(l, num_classes=readable_labels_size) for i,l in itol.items()}\n",
    "def ctoi(t):\n",
    "    for i,c in itoc.items():\n",
    "        if torch.equal(c,t):\n",
    "            return i\n",
    "    return -1\n",
    "itol, ltoi, itoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be121476-17ea-44ef-a761-733c28c1e681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13563, 64]) torch.Size([13563, 3])\n",
      "torch.Size([1633, 64]) torch.Size([1633, 3])\n",
      "torch.Size([3381, 64]) torch.Size([3381, 3])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 64\n",
    "\n",
    "def build_dataset(X, Y):\n",
    "    \n",
    "    Xt, Yt = [], []\n",
    "    \n",
    "    for x,y in zip(X, Y):\n",
    "        # padded context of zero tokens with block size expected\n",
    "        context = [0] * block_size\n",
    "        for i, w in enumerate(x):\n",
    "            try:\n",
    "              ix = stoi[w]\n",
    "            except KeyError:\n",
    "              ix = stoi[unk_tok]\n",
    "            if i > block_size // 2:\n",
    "              Xt.append(context)\n",
    "              Yt.append(itoc[ltoi[y]])\n",
    "            context = context[1:] + [ix] #crop and append\n",
    "        Xt.append(context)\n",
    "        Yt.append(itoc[ltoi[y]] if len(set(context)) > 1 else itoc[ltoi[0]])\n",
    "    Xt = torch.tensor(Xt)\n",
    "    Yt = torch.stack(Yt)\n",
    "    print(Xt.shape, Yt.shape)\n",
    "    return Xt, Yt\n",
    "\n",
    " \n",
    "Xtr, Ytr = build_dataset(Xtrw, Ytrw)      #80%\n",
    "Xdev, Ydev = build_dataset(Xdevw, Ydevw)  #10%\n",
    "Xte, Yte = build_dataset(Xtew, Ytew)      #10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76d15e53-12a7-4b6d-be97-b02c2c459bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> neutral\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> neutral\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n",
      "<U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> <U> ---> positive\n"
     ]
    }
   ],
   "source": [
    "# Pick ten samples in intervals to show the construction of XTr train set and YTr label set\n",
    "for x,y in zip(Xtr[:20], Ytr[:20]):\n",
    "    print(' '.join(itos[ix.item()] for ix in x), '--->', readable_labels[itol[ctoi(y)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c8384-8cbd-4a19-9a95-c44c656fff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # kaiming initialization for normalized variance\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# ---------------\n",
    "class BatchNorm1d:\n",
    "  def __init__(self, dim, eps=1e-6, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      dims = [d for d in range(0, x.dim() - 1)]\n",
    "      xmean = x.mean(tuple(dims), keepdim=True) # batch mean\n",
    "      xvar = x.var(tuple(dims), keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# ---------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# ---------------\n",
    "\n",
    "class Embedding:\n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "        \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "        \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# ---------------\n",
    "class FlattenConsecutive:\n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# ---------------\n",
    "class Sequential:\n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# ---------------\n",
    "\n",
    "class Softmax:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.softmax(x, dim=1)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ff73e-f51a-4318-8518-947765f3a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "wordset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe0056-16e3-43f9-9f4b-e6482544617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 24\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(wordset_size, n_embd),\n",
    "    FlattenConsecutive(4), Linear(n_embd * 4, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(4), Linear(n_hidden * 4, n_hidden // 2, bias=False), BatchNorm1d(n_hidden // 2), Tanh(),\n",
    "    FlattenConsecutive(4), Linear(n_hidden * 2, readable_labels_size)\n",
    "])\n",
    "\n",
    "    \n",
    "#parameter init\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].weight *= 0.1 #last linear layer is reduced so that we make it less confident at the beginning\n",
    "    \n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63b054-2fb6-4c2f-a0d4-f9a72130123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 120000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "lossv = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  \n",
    "  # forward pass\n",
    "  logits = model(Xb)\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.01 if i > max_steps/2 else 0.1\n",
    "\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    ix = torch.randint(0, Xdev.shape[0], (batch_size,))\n",
    "    Xv, Yv = Xdev[ix], Ydev[ix]\n",
    "    vlogits = model(Xv)\n",
    "    l = F.cross_entropy(vlogits, Yv)\n",
    "    lossv.append(l.log10().item())\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: loss(test): {loss.item():.4f} :loss(val): {l.item():.4f} : lr:{lr:.3f}' )\n",
    "    \n",
    "  lossi.append(loss.log10().item())\n",
    "  with torch.no_grad():\n",
    "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8ecc9-6860-468c-b9c8-c703aec964f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.out != None:\n",
    "      print(layer.__class__.__name__, ':', tuple(layer.out.shape))\n",
    "    else:\n",
    "      print(layer.__class__.__name__, ':', 'Uninitialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f64741-57e5-4c7c-8dd1-00dea4adcaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvi = torch.tensor(lossi).view(-1,1000).mean(1)\n",
    "lvp = torch.tensor(lossv).view(-1,1000).mean(1)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10), layout='constrained')\n",
    "ax.plot(lvi, label='training-loss')  \n",
    "ax.plot(lvp, label='validation-loss')  \n",
    "ax.set_title(\"Training and Validation Loss\") \n",
    "ax.legend();  # Add a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621424c-a53e-47f1-bb5c-03872599f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, ':', tuple(layer.out.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e4711-fef3-4f11-a108-1459983eaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(model.layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out.grad\n",
    "    if t != None:\n",
    "      print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "      hy, hx = torch.histogram(t, density=True)\n",
    "      plt.plot(hx[:-1].detach(), hy.detach())\n",
    "      legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('gradient distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7e108-19ae-4adc-b8b8-d1f7babea911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(model.layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh) or isinstance(layer, Softmax):\n",
    "    t = layer.out\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b11b3-c438-4e66-a9ca-7b1afae0fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  t = p.grad\n",
    "  if p.ndim == 2:\n",
    "    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends)\n",
    "plt.title('weights gradient distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7c400-5548-4fa0-b999-9a608dc03b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  if p.ndim == 2:\n",
    "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "    legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "plt.legend(legends);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede09db-ccfa-4e2b-9a2e-4dd87a4ba099",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51fb2ce-d1ff-4f75-b21f-1e7d334e4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "\n",
    "  logits = model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "split_loss('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83f2ad-9daa-4722-afc7-ece2c0154ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test_dataset(split):\n",
    "  X,Y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]  \n",
    "\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  trace = 5\n",
    "  for i in range(X.shape[0] // batch_size):\n",
    "    ix = torch.arange(i*batch_size, (i+1)*batch_size)\n",
    "    Xc, Yc = X[ix], Y[ix] # batch X,Y\n",
    "\n",
    "    logits = model(Xc)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "    target = []\n",
    "    for t in preds.tolist():\n",
    "      target.append(to_categorical(t, num_classes=readable_labels_size))\n",
    "    target = torch.stack(target)\n",
    "\n",
    "    for x,y,p,t in zip(Xc, Yc, probs, target):\n",
    "      act = readable_labels[itol[ctoi(y)]]\n",
    "      pred = readable_labels[itol[ctoi(t)]]\n",
    "      if act == pred:\n",
    "        correct += 1\n",
    "      total += 1\n",
    "      if total % 1000 == 0:\n",
    "        trace = 5\n",
    "      if trace > 0:\n",
    "        print(' '.join(itos[ix.item()] for ix in x), '\\n\\t---> Actual:', act, '---> Predicted:', pred, f' probabaility : {torch.max(p).item()*100:.2f}%',  )\n",
    "        trace -= 1\n",
    "  return correct, total\n",
    "\n",
    "correct,total = test_dataset('train')\n",
    "print(f'[TRAIN] Correct Sentiment: {correct} out of {total}, Accuracy : {correct/total * 100:.2f}%')\n",
    "\n",
    "correct,total = test_dataset('val')\n",
    "print(f'[VAL] Correct Sentiment: {correct} out of {total}, Accuracy : {correct/total * 100:.2f}%')\n",
    "\n",
    "correct,total = test_dataset('test')\n",
    "print(f'[TEST] Correct Sentiment: {correct} out of {total}, Accuracy : {correct/total * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b56bb-665b-46c5-b833-d4b12915e6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
